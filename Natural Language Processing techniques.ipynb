{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71aff002-c0b1-4c4e-ad75-d8cf6c5c1d7f",
   "metadata": {},
   "source": [
    "## Removal of Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0fc774-b361-4b89-83ee-6a25e2e7e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reached', 'support', 'got', 'helpful', 'response', 'within', 'minutes', 'impressed']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "word_list = ['reached', 'support', 'got', 'helpful', 'response', 'within', 'minutes', '!', '!', '!', '#', 'impressed']\n",
    "\n",
    "# Remove punctuation\n",
    "clean_tokens = [word for word in word_list if word not in string.punctuation]\n",
    "\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0df49-1be7-4185-a7ba-9d207daaa105",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e22dc783-8484-43b2-974a-25724eae1b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flying', 'lot', 'lately', 'flights', 'keep', 'getting', 'delayed', 'honestly', 'traveling', 'work', 'gets', 'exhausting', 'endless', 'delays', 'every', 'trip', 'teaches', 'something', 'new']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\23480\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\23480\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "review = \"\"\"\n",
    "I have been FLYING a lot lately and the Flights just keep getting DELAYED. Honestly, traveling for WORK gets exhausting with endless delays,\n",
    "but every trip teaches you something new!\n",
    "\"\"\"\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Lowercase the review\n",
    "lower_text = review.lower()\n",
    "\n",
    "# Tokenize the lower_text into words\n",
    "tokens = nltk.word_tokenize(lower_text)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "clean_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "print(clean_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7135243-1a60-4f9e-af48-1dae4de59a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "OMG 沽ｱ!!! I can't believe this flight got delayed AGAIN... 沽､笨茨ｸ十n",
      "Check out my blog at https://mytravelblog.com 笨ｨ笨茨ｸ十n",
      "Traveling for work is exhausting, but every trip teaches you something new!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "OMG 沽ｱ!!! I can't believe this flight got delayed AGAIN... 沽､笨茨ｸ十n",
    "Check out my blog at https://mytravelblog.com 笨ｨ笨茨ｸ十n",
    "Traveling for work is exhausting, but every trip teaches you something new!\n",
    "\"\"\"\n",
    "print(\"Original Text:\\n\", review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a84ff-2afd-4705-89f5-6a7247b47075",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb4e5cc7-ee81-4f4e-ba7e-b5212cb2ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without URLs:\n",
      " \n",
      "OMG 沽ｱ!!! I can't believe this flight got delayed AGAIN... 沽､笨茨ｸ十n",
      "Check out my blog at  笨ｨ笨茨ｸ十n",
      "Traveling for work is exhausting, but every trip teaches you something new!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_url_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', review)\n",
    "print(\"Text without URLs:\\n\", no_url_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eef2c1-a500-4e70-84c2-0b59f0234908",
   "metadata": {},
   "source": [
    "## Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80486824-422e-40ed-84a5-10575753e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Emojis:\n",
      " \n",
      "OMG !!! I can't believe this flight got delayed AGAIN... \n",
      "Check out my blog at  \n",
      "Traveling for work is exhausting, but every trip teaches you something new!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"\n",
    "    u\"\\U0001F300-\\U0001F5FF\"\n",
    "    u\"\\U0001F680-\\U0001F6FF\"\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "no_emoji_text = emoji_pattern.sub(r'', no_url_text)\n",
    "print(\"Text without Emojis:\\n\", no_emoji_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553be27-c01c-41af-a9f7-b908461eb8f7",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb797401-bc92-42ad-9739-cbc2a88978a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words:\n",
      " ['OMG', '!', '!', '!', 'I', 'ca', \"n't\", 'believe', 'this', 'flight', 'got', 'delayed', 'AGAIN', '...', 'Check', 'out', 'my', 'blog', 'at', 'Traveling', 'for', 'work', 'is', 'exhausting', ',', 'but', 'every', 'trip', 'teaches', 'you', 'something', 'new', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(no_emoji_text)\n",
    "print(\"Tokenized Words:\\n\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6ff7e-105d-4ceb-9273-685729e13187",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8db9f803-e921-4686-948d-2e7af0a33221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\23480\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43f94b6e-3dd9-43de-9c5c-b7ea91f023c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      " ['OMG', '!', '!', '!', 'I', 'ca', \"n't\", 'believe', 'this', 'flight', 'got', 'delayed', 'AGAIN', '...', 'Check', 'out', 'my', 'blog', 'at', 'Traveling', 'for', 'work', 'is', 'exhausting', ',', 'but', 'every', 'trip', 'teach', 'you', 'something', 'new', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmatized Tokens:\\n\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ac02c-406d-4dde-a139-9799e95f0f42",
   "metadata": {},
   "source": [
    "## Note: I didn't remove the punctuatoins and stop words. Now, let me remove them and lemmmatize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3097914a-4ea6-4d91-8fe2-aa2c0db1aea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Tokens:\n",
      " ['OMG', 'I', 'ca', \"n't\", 'believe', 'flight', 'got', 'delayed', 'AGAIN', '...', 'Check', 'blog', 'Traveling', 'work', 'exhausting', 'every', 'trip', 'teaches', 'something', 'new']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "clean_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "print(\"Clean Tokens:\\n\", clean_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d883bdf-a34f-4cf9-9484-03fb206d644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      " ['OMG', 'I', 'ca', \"n't\", 'believe', 'flight', 'got', 'delayed', 'AGAIN', '...', 'Check', 'blog', 'Traveling', 'work', 'exhausting', 'every', 'trip', 'teach', 'something', 'new']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in clean_tokens]\n",
    "print(\"Lemmatized Tokens:\\n\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c559b5d-b940-4168-8660-a19b1441ede4",
   "metadata": {},
   "source": [
    "### Let's view our initial text once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc69478f-a768-46fa-b244-ecd1bebbdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OMG 沽ｱ!!! I can't believe this flight got delayed AGAIN... 沽､笨茨ｸ十n",
      "Check out my blog at https://mytravelblog.com 笨ｨ笨茨ｸ十n",
      "Traveling for work is exhausting, but every trip teaches you something new!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475dffc-04a1-49d2-85a2-4a4b0aa12e57",
   "metadata": {},
   "source": [
    "We see that we have actually proecessed this text using different tecniques and this shows how these techniques are really unique in natural Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c7143-7b23-4670-9c4b-01cdda5f15c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
